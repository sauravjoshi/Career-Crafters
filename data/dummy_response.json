[
  {
    "Match Distance": 0.12917691469192505,
    "city": "Houston",
    "company": "Fluence",
    "job level": "Mid senior",
    "job_link": "https://www.linkedin.com/jobs/view/senior-backend-engineer-data-engineer-at-fluence-3774813683",
    "job_location": "Houston, TX",
    "job_skills": "Python, Go, C++, Rust, PostgreSQL, TimescaleDB, Data Lakes, Version control, Code reviews, Continuous integration, Deployment, Testing frameworks, Automation tools, Jenkins, TestRail, SQL, NoSQL, Data pipelines, Data modeling, Data governance, Data cataloging, Schema management, Data discovery, Data quality, Data integration, Data science, Machine learning, Artificial intelligence, Energy storage, Renewables, Optimization, Software development, Backend programming, Software engineering, Load testing",
    "job_summary": "About Fluence:\nFluence Energy, Inc. (Nasdaq: FLNC) is a global market leader in energy storage products and services, and optimization software for renewables and storage. With a presence in over 47 markets globally, Fluence provides an ecosystem of offerings to drive the clean energy transition, including modular, scalable energy storage products, comprehensive service offerings, and the Fluence IQ Platform, which delivers AI-enabled SaaS products for managing and optimizing renewables and storage from any provider. Fluence is transforming the way we power our world by helping customers create more resilient and sustainable electric grids.\nFor more information, visit our website , or follow us on LinkedIn or Twitter . To stay up to date on the latest industry insights, sign up for Fluence's Full Potential Blog .\nOUR CULTURE AND VALUES\nWe are guided by our passion to transform the way we power our world. Achieving our goals requires creativity, diversity of ideas and backgrounds, and building trust to effect change and move with speed.\nWe are Leading\nFluence currently has thousands of MW of energy storage projects operated or awarded worldwide in addition to the thousands of MW of projects managed by our trading platform\u2014and we are growing every day.\nWe are Responsible\nFluence is defined by its unwavering commitment to safety, quality, and integrity.\nWe are Agile\nWe achieve our goals and meet our customer\u2019s needs by cultivating curiosity, adaptability, and self-reflection in our teams.\nWe are Fun\nWe value the diversity in thought and experience of our coworkers and customers. Through honest, forthcoming, and respectful communications we work to ensure that Fluence is an inclusive and welcoming environment for all.\nAs a Senior Backend Software Engineer / Data Engineer, you will be responsible for developing and maintaining the backend systems and data infrastructure for a large grid-scale storage energy company. Your role will involve working closely with our software engineering teams to optimize access to operational data and design and implement data models and APIs that work well for our user-facing application. You will also work heavily with our data science teams to build and optimize data pipelines, integrate data sets, and ensure data quality and governance.\nWhat does a Senior Backend Engineer/Data Engineer do at Fluence?\nBuild and optimize data pipelines to ensure efficient data integration and flow across various sources and destinations.\nImplement and maintain automated testing and deployment processes to ensure the reliability and scalability of the backend systems.\nOptimize the performance and scalability of the backend systems to handle large-scale data processing and storage requirements, and design load testing to test and prove systems before field deployments.\nCollaborate with cross-functional teams to design and implement data models that support product features and data-driven decision-making.\nDevelop and maintain backend systems for data cataloging, schema management, and data discovery to facilitate effective data governance and access.\nWhat does the ideal candidate look like?\nBachelor's or Master's degree in Computer Science, Engineering, or a related field.\n5+ years of experience working on high-traffic systems, optimizing performance, and implementing scalable solutions to meet business needs effectively.\nStrong knowledge of backend programming languages such as Python, Go, C++, and Rust.\nExperience with database systems, including PostgreSQL, TimescaleDB as well as interacting with larger, more static stores such as data lakes.\nStrong understanding of professional software development practices, including version control, code reviews, and continuous integration and deployment.\nFamiliarity with testing frameworks and automation tools for backend software development, such as Jenkins and TestRail.\nExcellent problem-solving and communication skills, with the ability to work effectively in a collaborative team environment.\nExperience in the energy industry is a plus, but not a must.\n$0 - $0 a year\nFluence IS AN EQUAL OPPORTUNITY EMPLOYER and fully subscribes to the principles of Equal Employment Opportunity to ensure that all applicants and employees are considered for hire, promotion, and job status without regard to race, color, religion, sex, national origin, age, disability, veteran status, sexual orientation, marital or familial status.\nShow more\nShow less",
    "job_title": "Senior Backend Engineer / Data Engineer",
    "job_type": "Hybrid",
    "token_number_after_lem": 465
  },
  {
    "Match Distance": 0.13697504997253418,
    "city": "Brisbane",
    "company": "Mastercard",
    "job level": "Mid senior",
    "job_link": "https://au.linkedin.com/jobs/view/lead-software-engineer-at-mastercard-3782453913",
    "job_location": "Brisbane, Queensland, Australia",
    "job_skills": "Java, SOA, Spring Boot, Steeltoe, Angular, DXP, OWASP, PCI DSS, Advanced design patterns, Gang of four, Jenkins, Bamboo, AWS/Azure pipelines, XL Release, Sonar, Checkmarx, Nexus, JFrog XRay, Veracode",
    "job_summary": "Our Purpose\nWe work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team \u2013 one that makes better decisions, drives innovation and delivers better business results.\nTitle And Summary\nLead Software Engineer\nOverview\nJob Description Summary\nThis role is part of the Software Engineering Group within the Mastercard Payment Gateway Services division in Brisbane, collaborating with internal customers across the globe.\nWe are looking for talented developers with experience in designing commercial Java-based, distributed, highly available applications, where system performance is critical.\nDo you have retail payments/e-commerce industry experience and looking to try something new in 2023?\nAre you motivated to be a part of driving a world beyond cash?\nRole\nProvide technical expertise for payment applications, which include secure, mission critical transaction processing systems.\nResponsible for building commercial Java-based, SOA, highly available applications, where system performance is critical.\nCollaborate with team members and internal stakeholders to ensure requirements meet customer needs.\nProduce innovative technical solutions to meet business and product strategies.\nProvide training, mentoring and guidance, to increase the skill of other Software Engineering team members\nProvide development and architecture guidance to team members\nDefine, design, and develop procedures and solutions to meet the business requirements/enhancements\nDrive prioritization decisions and trade-offs in working with product partners and architecture group\nIntroduce new technologies and architecture by following enterprise guidelines\nIdentify opportunities and build roadmaps to enhance the Payment Gateway platform\nAll About You\nBachelor's degree in Information Systems, Information Technology, Computer Science or Engineering or equivalent work experience.\nExperience developing large scale Java services and/or web user interfaces.\nKnowledge of security concerns such as OWASP Top 10 and PCI DSS.\nEnjoy working with other developers to solve difficult problems.\nHas the ability to write secure code in multiple languages (Java, C, C++) and familiar with secure coding standards (e.g., OWASP, CWE, SEI CERT) and vulnerabilities\nHas skills in building applications using open frameworks to achieve reuse and reduce development times (e.g., Spring Boot, Steeltoe, Angular, DXP, others)\nAble to perform debugging and troubleshooting to analyze core, heap, thread dumps and remove coding errors\nHas skills to document and coach team on the development practices and coding guidelines (e.g., branching, peer reviews, library use, logging, scanning rules, test-driven development, error handling)\nUnderstands use cases for advanced design patterns (e.g., service-to-worker, MVC, API gateway, intercepting filter, dependency injection, lazy loading, all from the gang of four) to implement efficient code\nHas skills to undertake a technical review of code across applications and their dependencies to look for anti-patterns and promote continuous refactoring\nUnderstands and elaborates technical debt and operational issues to drive prioritization discussions with stakeholders to improve the run experience\nUnderstands system architecture to plan for platform and infrastructure capacity (e.g., database, compute, network, storage) and drives the dependency prioritization to reduce the delivery lead time\nHas skills to understand customer journeys and ensure a Mastercard good experience by continuously reducing Mean time to mitigate (MTTM) for incidents and ensuring high availability (99.95% as a starting point)\nHas skills to simplify deployment and eliminate software and infrastructure snowflakes using standardized platforms, ephemeral instances, and automation\nHas skills to orchestrate release workflows and pipelines and apply standardized pipelines via APIs to achieve CI and CD using industry-standard tools (e.g., Jenkins, Bamboo, AWS/Azure pipelines, XL Release, others)\nAble to configure rules and build automation for code with vulnerability scanning and software composition analysis using standard tools (e.g., Sonar, Checkmarx, Nexus, JFrog XRay, Veracode, others)\nHas skills to define, organize, and report on test runs for major, minor, and hotfix releases (including unit, component level, system level, customer journeys, past customer issues, and regulatory controls)\nHas skills to conduct various performance tests (e.g., load, spike, breakpoint, endurance) to Understands application/service limits and behaviors\nCorporate Security Responsibility\nAll activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:\nAbide by Mastercard\u2019s security policies and practices;\nEnsure the confidentiality and integrity of the information being accessed;\nReport any suspected information security violation or breach, and\nComplete all periodic mandatory security trainings in accordance with Mastercard\u2019s guidelines.\nShow more\nShow less",
    "job_title": "Lead Software Engineer",
    "job_type": "Hybrid",
    "token_number_after_lem": 557
  },
  {
    "Match Distance": 0.14096462726593018,
    "city": "Phoenix",
    "company": "Patterned Learning Career",
    "job level": "Mid senior",
    "job_link": "https://www.linkedin.com/jobs/view/junior-data-engineer-us-at-patterned-learning-career-3790448739",
    "job_location": "Phoenix, AZ",
    "job_skills": "Python, Java, SQL, ETL, Data Modeling, Data Quality Assessment, Data Cleansing, Data Validation, PySpark, Databricks, Azure Synapse, Kafka, Neural Networks, Deep Learning",
    "job_summary": "This is a remote position.\njunior Data Engineer (US)\n- Remote Job, 1+ Year Experience\nAnnual Income:\n$55K - $65K\nAbout us:\nPatterned Learning is a platform that aims to help developers code faster and more efficiently. It offers features such as collaborative coding, real-time multiplayer editing, and the ability to build, test, and deploy directly from the browser. The platform also provides tightly integrated code generation, editing, and output capabilities.\nPosition Summary\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance the efficiency, reliability, and performance of CVS Health\u2019s IT operations.\nKey Responsibilities include:\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation.\nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency\nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency\nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data\nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently\nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation\nCreate/maintain documentation for data processes, data flows, and system configurations\nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness\nCharacteristics of this role:\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful.\nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills.\nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas.\nAttention to detail: Systematically and accurately research future solutions and current problems.\nStrong work ethic: The innate drive to do work extremely well.\nPassion: A drive to deliver better products and services than expected to customers.\nRequired Qualifications\n2+ years of programming experience in languages such as Python, Java, SQL\n2+ years of experience with ETL tools and database management (relational, non-relational)\n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures\nSkills in data quality assessment, data cleansing, and data validation\nPreferred Qualifications\nKnowledge of big data technologies and cloud platforms\nExperience with technologies like PySpark, Databricks, and Azure Synapse.\nEducation\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\nWhy Patterned Learning LLC?\nPatterned Learning can provide intelligent suggestions, automate repetitive tasks, and assist developers in writing code more effectively. This can help reduce coding errors, improve productivity, and accelerate the development process.\nThe pattern recognition is particularly relevant in the context of coding. Neural networks, especially deep learning models, are commonly employed for pattern detection and classification tasks. These models simulate human decision-making and can identify patterns in data, making them well-suited for tasks like code analysis and generation.\nShow more\nShow less",
    "job_title": "junior Data Engineer (US)",
    "job_type": "Remote",
    "token_number_after_lem": 390
  },
  {
    "Match Distance": 0.14306044578552246,
    "city": "Cambridge",
    "company": "Flagship Pioneering",
    "job level": "Mid senior",
    "job_link": "https://www.linkedin.com/jobs/view/senior-data-scientist-at-flagship-pioneering-3790392128",
    "job_location": "Cambridge, MA",
    "job_skills": "Data management, Data architecture, Statistical analysis, Cloud computing, System networking, API generation, Ontology management, Database schema, Query optimization, Knowledge graphs, Security protocols, Data visualization, Quality control methods, Online analysis algorithms, Data science, Data engineering, Data governance, Data modeling, Data integration, Data warehousing, Data lakes, Data pipelines, Big data, Artificial intelligence, Machine learning, SQL, NoSQL, RDBMS, ODS, Data marts, AWS, S3, RDS, Neptune, Web security, Permissions management, Eventbased architectures, Multiclient event bus, Nextflow, Airflow",
    "job_summary": "Company Summary\nFL96, Inc. is a privately held, early-stage company developing a novel AI and data-driven approach to materials discovery and development to accelerate the transition to a sustainable future.\nFL96 was founded by Flagship Pioneering, an innovation enterprise that conceives, creates, resources, and builds companies that invent breakthrough technologies to transform health care, agriculture, and sustainability. Flagship has created over 100 groundbreaking companies since 2000, including Moderna (NASDAQ: MRNA), Generate Biomedicines, Tessera Therapeutics, Indigo Ag, Inari Agriculture, and Sana Biotechnology.\nPosition Summary\nIn this position, you will establish and manage a state-of-the-art data management system that interfaces between experimental workflows, computational workflows, and machine learning algorithms. You will ensure the security of the data while allowing easy access for human and AI researchers. You will help create and implement data visualization capabilities, quality control methods, and on-line analysis algorithms to facilitate platform efforts.\nThe acceleration of materials discovery and translation to sustainability technologies hinges upon a foundation in data science and data management. Building this data management system will require incorporating and advancing best practices across statistical analysis, cloud computing, system networking, API generation, ontology management, database schema, query optimization, knowledge graphs, and security protocols.\nKey Responsibilities:\nDevelop and oversee data architecture, vision, & modeling efforts using industry data platforms, including the design of data structures and the development of business transformation logic.\nBuild and maintain target architectures for new data products including database design, data processing, and data integrations.\nCollaborate with the computational and experimental teams to create optimized, reusable, and scalable semantic models, complete with metadata and lineage information that describes the data model, data structure, and semantics\nDevelop on-prem and/or cloud native RDBMS, operational data store (ODS), data marts, and data lakes on target platforms (SQL/NoSQL)\nDefine and govern data modeling and design standards, tools, best practices, and related development for enterprise data models.\nStay current on technology trends such as database technologies, high performance computing, infrastructure as code and communicate their value and applicability to the team.\nScope and develop proof of concepts to demonstrate feasibility of new data driven solutions.\nAccelerate adoption of the latest technology trends within cloud and big data solutions.\nMentor operations and business teams on data architecture design and requirements.\nRequired Qualifications\nBS, MS, or PhD in Computer Science or related scientific field with 2+ years' experience, or master's degree with 5+ years' experience.\nExperience designing and implementing modern data architectures where data stores and knowledge graphs are integrated with real-time data streams\nExperience establishing ontologies, graph, and SQL database schema\nKnowledge and implementation of transactional, object, document, and graph databases\nExperience with data security, user management, and disaster recovery\nMultiple examples of collaborating with stakeholders to build data pipelines for ingesting high throughput experimental data and to enable automation\nUnderstanding of the AI/Machine learning lifecycle and model refinement from high volume, real-time data streams\nStrong self-starter and independent thinker, with strong attention to detail\nExcellent communication and presentation skills, capable of conveying technical information in a clear and thorough manner\nEager to work with highly skilled and dynamic teams in a fast-paced, entrepreneurial, and technical setting\nPreferred Qualifications\nExperience leveraging cloud computing services, including AWS services such as S3, RDS, and Neptune\nExperience with web security and permissions management tools for enabling secure real-time data communication\nExperience creating and managing event-based cloud architectures including a multi-client event bus\nExperience building enterprise data governance programs\nExperience designing and building data processing pipelines using Nextflow, Airflow, or similar technologies\nAbout Flagship\nFlagship Pioneering is a biotechnology company that invents and builds platform companies, each with the potential for multiple products that transform human health or sustainability. Since its launch in 2000, Flagship has originated and fostered more than 100 scientific ventures, resulting in more than $90 billion in aggregate value. Many of the companies Flagship has founded have addressed humanity\u2019s most urgent challenges: vaccinating billions of people against COVID-19, curing intractable diseases, improving human health, preempting illness, and feeding the world by improving the resiliency and sustainability of agriculture. Flagship has been recognized twice on FORTUNE\u2019s \u201cChange the World\u201d list, an annual ranking of companies that have made a positive social and environmental impact through activities that are part of their core business strategies, and has been twice named to Fast Company\u2019s annual list of the World\u2019s Most Innovative Companies. Learn more about Flagship at www.flagshippioneering.com.\nFlagship Pioneering and our ecosystem companies are\ncommitted to equal employment opportunity\nregardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.\nAt Flagship, we recognize there is no perfect candidate. If you have some of the experience listed above but not all, please apply anyway. Experience comes in many forms, skills are transferable, and passion goes a long way. We are dedicated to building diverse and inclusive teams and look forward to learning more about your unique background.\nRecruitment & Staffing Agencies\n: Flagship Pioneering and its affiliated Flagship Lab companies (collectively, \u201cFSP\u201d) do not accept unsolicited resumes from any source other than candidates. The submission of unsolicited resumes by recruitment or staffing agencies to FSP or its employees is strictly prohibited unless contacted directly by Flagship Pioneering\u2019s internal Talent Acquisition team. Any resume submitted by an agency in the absence of a signed agreement will automatically become the property of FSP, and FSP will not owe any referral or other fees with respect thereto.\nShow more\nShow less",
    "job_title": "(Senior) Data Scientist",
    "job_type": "Onsite",
    "token_number_after_lem": 672
  },
  {
    "Match Distance": 0.14307010173797607,
    "city": "Denver",
    "company": "Mevi",
    "job level": "Mid senior",
    "job_link": "https://www.linkedin.com/jobs/view/software-engineer-apis-and-services-at-mevi-3296750036",
    "job_location": "Denver, CO",
    "job_skills": "Go, APIs, Microservices, Databases, Data pipeline, Pub/sub frameworks, External systems, Stateless microservices, Container orchestration, Auto devops, Realtime, MVP, Datadriven, Software development, Backend, SaaS, Iterative software development, Quality code, Flexibility, Agility, Collaboration",
    "job_summary": "The Challenge\nYou will help design and build fast, efficient, and scalable software that powers innovative business applications.\nYou will use go to build APIs and microservices.\nYou will integrate databases, data pipeline and pub/sub frameworks, and complex external systems and APIs.\nYou will learn about stateless microservices, container orchestration, and auto devops.\nYou will work hard, have fun, innovate, and deliver real products to real customers that provide real value every day.\nThe Requirements\nYou have 3-10 years of meaningful back-end software development experience.\nYou have relevant, recent, and meaningful experience building SaaS software with go.\nYou have relevant and meaningful experience building data-driven APIs and microservices.\nYou have a comprehensive understanding of iterative software development processes.\nYou deliver quality code that is easy to read and painless to test.\nYou believe that flexibility, agility, and releasing fast and frequently wins the battle and the war.\nYou are a self-motivated creative and vocal thinker with a good balance of exceptionally high IQ and EQ.\nYou believe that building software is a team sport. You are always a team player and frequently an MVP.\nYou use a lot of other software, every day, even if you don\u2019t need to, because you are genuinely interested in how other software engineers are pushing the limits of innovation.\nThe Perks\nCompetitive salary\nplus\nequity.\nCompetitive health, dental, and vision insurance.\nFlexible vacation policy.\nWhat is Mevi?\nMevi, a SaaS provider of a digital transaction platform for real estate brokerages, agents and their clients. Mevi creates a paradigm shift in the usability and accessibility of documents and data. And Mevi is the only platform that\nempowers\nbrokerages, enabling them to take control of their brand, governance, and data.\nReal estate brokerages and agents lack modern and efficient tools for managing transactions from offer to close. They \u201cswivel chair\u201d from their front/back office software solutions to use antiquated and siloed transaction management tools that are limited to form filling and signing, are not mobile friendly, and lack collaboration features. Due diligence and client and vendor participation is limited to emails, text, and phone calls. And compliance auditing is a manual and reactive process when deals need to close.\nThese existing solutions try to make real estate documents feel more digital by wrapping them with more complex UI widgets to support more business logic. But this approach leaves brokerages and agents with little to no control or customization capabilities and keeps the valuable transaction data locked up in the resulting PDF files.\nMevi Smart Contract technology turns real estate documents into pure data, creating a digital representation of the document and how it is used so that the UI and business logic are truly data-driven, presented simply, and powerful underneath. This approach frees our software from the limitations of attorney defined document structures and enables Mevi to provide simple, complete, and modern software that delivers time savings, process control and customization, client and vendor engagement, collaborative due diligence, and proactive compliance.\nMevi is a neutral technology provider with a white-label offering and an extensible and modular platform. This uniquely positions Mevi to partner and integrate throughout the ecosystem. And in the near future, similar to how Stripe enables any software to accept payments using a simple API, Mevi can enable any software to provide modern, data-driven, and customizable transaction management.\nwww.mevi.io\nShow more\nShow less",
    "job_title": "Software Engineer - APIs and Services",
    "job_type": "Onsite",
    "token_number_after_lem": 380
  }
]
